{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZElJE4UV1GL"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Укажи свой путь до папки с датасетом\n",
        "base_dir = Path(\"C:/Users/user/Desktop/slovo\")\n",
        "csv_path = base_dir / \"annotations.csv\"\n",
        "json_dir = base_dir / \"json\"\n",
        "json_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Загрузка CSV с аннотациями\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Разделение на train и dev по полю 'train'\n",
        "train_data = {}\n",
        "dev_data = {}\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    record = {\n",
        "        \"name\": row['attachment_id'],\n",
        "        \"text\": row['text'],\n",
        "        \"video_path\": f\"{row['attachment_id']}.mp4\",\n",
        "        \"pose\": f\"{row['attachment_id']}.pkl\"\n",
        "    }\n",
        "    if row['train']:\n",
        "        train_data[row['attachment_id']] = record\n",
        "    else:\n",
        "        dev_data[row['attachment_id']] = record\n",
        "\n",
        "# Сохранение в JSON\n",
        "with open(json_dir / \"slovo_train.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "with open(json_dir / \"slovo_dev.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(dev_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"JSON-файлы сохранены в:\", json_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "args = SimpleNamespace(\n",
        "    dataset=\"CSL_News\",         # заглушка: влияет только на структуру путей в S2T_Dataset_news\n",
        "    task=\"SLT\",                 # задача — перевод жестов в текст (Sign Language Translation)\n",
        "    output_dir=\"C:/Users/user/Desktop/slovo/output\",  # куда сохранять модель и логи\n",
        "    finetune=\"\",\n",
        "    batch_size=4,\n",
        "    num_workers=2,\n",
        "    pin_mem=True,               # ускорение передачи данных на GPU\n",
        "    seed=42,                    # фиксируем сид для воспроизводимости\n",
        "    distributed=False,         # отключаем распределённое обучение\n",
        "    hidden_dim=256,            # размер скрытого признакового пространства\n",
        "    max_length=32,             # максимальная длина последовательности кадров (T)\n",
        "    rgb_support=False,         # работаем только с позами (без RGB)\n",
        "    gradient_accumulation_steps=2,\n",
        "    gradient_clipping=1.0,\n",
        "    offload=False,             # не используем offload\n",
        "    dtype=\"bf16\",\n",
        "    zero_stage=2,\n",
        "    quick_break=0,\n",
        "    warmup_epochs=0,\n",
        "    epochs=10,\n",
        "    eval=False,\n",
        "    label_smoothing=0.1,\n",
        "    lr=1e-4,\n",
        "    min_lr=1e-6\n",
        ")\n"
      ],
      "metadata": {
        "id": "_I4RzbRyWgYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from datasets import S2T_Dataset_news\n",
        "\n",
        "# Пути к аннотациям\n",
        "base_dir = Path(\"C:/Users/user/Desktop/slovo\")\n",
        "train_json = base_dir / \"json/slovo_train.json\"\n",
        "dev_json = base_dir / \"json/slovo_dev.json\"\n",
        "\n",
        "# Загрузка датасетов\n",
        "train_dataset = S2T_Dataset_news(train_json, args, phase=\"train\")\n",
        "dev_dataset = S2T_Dataset_news(dev_json, args, phase=\"dev\")\n",
        "\n",
        "print(f\"Размер обучающей выборки: {len(train_dataset)}\")\n",
        "print(f\"Размер валидационной выборки: {len(dev_dataset)}\")\n"
      ],
      "metadata": {
        "id": "rR1ieMVwW181"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, SequentialSampler\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    shuffle=True,                           #обязательно перемешать train\n",
        "    num_workers=args.num_workers,\n",
        "    collate_fn=train_dataset.collate_fn,   #кастомная функция объединения батча\n",
        "    pin_memory=args.pin_mem,\n",
        "    drop_last=True                         #убрать последний неполный батч\n",
        ")\n",
        "\n",
        "dev_loader = DataLoader(\n",
        "    dev_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    sampler=SequentialSampler(dev_dataset),  # без перемешивания для валидации\n",
        "    num_workers=args.num_workers,\n",
        "    collate_fn=dev_dataset.collate_fn,\n",
        "    pin_memory=args.pin_mem\n",
        ")\n"
      ],
      "metadata": {
        "id": "BZj1GN-EXFHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from timm.optim import create_optimizer\n",
        "from transformers import get_scheduler\n",
        "\n",
        "# Создаём оптимизатор для модели\n",
        "optimizer = create_optimizer(args, model)\n",
        "\n",
        "# Планировщик learning rate с косинусным спадом\n",
        "lr_scheduler = get_scheduler(\n",
        "    name='cosine',\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=args.epochs * len(train_loader),\n",
        ")\n"
      ],
      "metadata": {
        "id": "XxaM2wDYX1Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import time\n",
        "import datetime\n",
        "import json\n",
        "from models import get_requires_grad_dict\n",
        "import utils\n",
        "\n",
        "output_dir = Path(args.output_dir)\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "max_bleu = 0  # максимальный BLEU-4 по ходу обучения\n",
        "start_time = time.time()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "F3TQaBlJYK77",
        "outputId": "9bda1c24-adfd-47fd-bda6-d2e70aac0cc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'models'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-938cfecbe811>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_requires_grad_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pre_training import train_one_epoch, evaluate\n",
        "\n",
        "for epoch in range(args.epochs):\n",
        "    # Обучение на одной эпохе\n",
        "    train_stats = train_one_epoch(args, model, train_loader, optimizer, epoch, model_without_ddp=model)\n",
        "\n",
        "    # Оценка BLEU-4 на dev-наборе\n",
        "    test_stats = evaluate(args, dev_loader, model, model)\n",
        "\n",
        "    # Если метрика BLEU улучшилась — сохраняем чекпойнт\n",
        "    if test_stats[\"bleu4\"] > max_bleu:\n",
        "        max_bleu = test_stats[\"bleu4\"]\n",
        "        best_ckpt = output_dir / \"best_checkpoint.pth\"\n",
        "        utils.save_on_master({\"model\": get_requires_grad_dict(model)}, best_ckpt)\n",
        "\n",
        "    # Выводим BLEU-4 для текущей эпохи\n",
        "    print(f\"Epoch {epoch} — BLEU-4: {test_stats['bleu4']:.2f} — Max BLEU-4: {max_bleu:.2f}\")\n"
      ],
      "metadata": {
        "id": "Jf_u-kHuYrPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    log_stats = {\n",
        "        **{f'train_{k}': v for k, v in train_stats.items()},\n",
        "        **{f'test_{k}': v for k, v in test_stats.items()},\n",
        "        'epoch': epoch,\n",
        "        'n_parameters': utils.count_parameters_in_MB(model)\n",
        "    }\n",
        "\n",
        "    with (output_dir / \"log.txt\").open(\"a\") as f:\n",
        "        f.write(json.dumps(log_stats) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "wqO7aMVSZNiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_time = time.time() - start_time\n",
        "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "print(\"Общее время обучения:\", total_time_str)\n"
      ],
      "metadata": {
        "id": "WLTDgKXcZZkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, SequentialSampler\n",
        "from datasets import S2T_Dataset_news\n",
        "\n",
        "# Повторно определим dev-набор\n",
        "dev_dataset = S2T_Dataset_news(dev_json, args, phase=\"dev\")\n",
        "dev_loader = DataLoader(\n",
        "    dev_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    sampler=SequentialSampler(dev_dataset),\n",
        "    num_workers=args.num_workers,\n",
        "    collate_fn=dev_dataset.collate_fn,\n",
        "    pin_memory=args.pin_mem\n",
        ")\n"
      ],
      "metadata": {
        "id": "JiqWwWPtZae7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models import Uni_Sign\n",
        "import torch\n",
        "\n",
        "# Заново инициализируем модель и загружаем лучшие веса\n",
        "model = Uni_Sign(args).cuda()\n",
        "checkpoint = torch.load(output_dir / \"best_checkpoint.pth\", map_location=\"cuda\")\n",
        "model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "fhUw5RxGZ4Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "assert len(tgt_refs) == len(tgt_pres)\n",
        "\n",
        "y_true = [ref.strip().lower() for ref in tgt_refs]\n",
        "y_pred = [pred.strip().lower() for pred in tgt_pres]\n",
        "\n",
        "classes = sorted(list(set(y_true) | set(y_pred)))\n",
        "\n",
        "# Генерация отчёта\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_true, y_pred, labels=classes, digits=2))\n"
      ],
      "metadata": {
        "id": "NGmo9yIFhWNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "y_true = [ref.strip().lower() for ref in tgt_refs]\n",
        "y_pred = [pred.strip().lower() for pred in tgt_pres]\n",
        "\n",
        "all_classes = sorted(list(set(y_true) | set(y_pred)))\n",
        "\n",
        "# Строим матрицу ошибок\n",
        "cm = confusion_matrix(y_true, y_pred, labels=all_classes)\n",
        "df_cm = pd.DataFrame(cm, index=all_classes, columns=all_classes)\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Вывод матрицы\n",
        "print(\"\\nConfusion Matrix:\\n\")\n",
        "print(df_cm.to_string())\n"
      ],
      "metadata": {
        "id": "EeJMJfpnhzL3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}